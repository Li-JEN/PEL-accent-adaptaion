{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/espnet/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "class china_accent(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        data = {}\n",
    "        with open(os.path.join(self.path,\"wav.scp\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, wav_path = each.split(\" \")[0], each.split(\" \")[1].strip(\"\\n\")\n",
    "                data[id] = [wav_path]\n",
    "        with open(os.path.join(self.path,\"text\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, text = each.split(\"\\t\")[0], each.split(\"\\t\")[1].strip(\"\\n\")\n",
    "                data[id] += [text]\n",
    "        self.n_samples = len(data.keys())\n",
    "        self.data = np.array(list(data.values()))\n",
    "        # print(self.data)\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return wav_path, text\n",
    "        '''\n",
    "        # speech_array, sampling_rate = librosa.load(self.data[index][0], sr=16_000)\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test_dataset(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,path, china_path):\n",
    "        self.path = path\n",
    "        self.china_path = china_path ## for char not phn\n",
    "        data = {}\n",
    "        # with open(os.path.join(self.path,\"wav.scp\"),'r') as f:\n",
    "        #     all =  f.readlines()\n",
    "        #     for each in all:\n",
    "        #         id, wav_path = each.split(\" \")[0], each.split(\" \")[1].strip(\"\\n\")\n",
    "        #         data[id] = [wav_path]\n",
    "        for each in os.listdir(self.path):\n",
    "            # print(each)\n",
    "            if \".wav\" in  each:\n",
    "                id, wav_path = each.replace(\".wav\",\"\"), os.path.join(self.path,each)\n",
    "                data[id] = [wav_path]\n",
    "        with open(os.path.join(self.china_path,\"text\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, text = each.split(\"\\t\")[0], each.split(\"\\t\")[1].strip(\"\\n\")\n",
    "                data[id] += [text]\n",
    "        self.n_samples = len(data.keys())\n",
    "        self.data = np.array(list(data.values()))\n",
    "        # print(self.data)\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return wav_path, text\n",
    "        '''\n",
    "        # speech_array, sampling_rate = librosa.load(self.data[index][0], sr=16_000)\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"/home/espnet/egs2/csmsc/tts1/data/eval1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_FT = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_FT = DataLoader(dataset=dataset_FT, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_DFT = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_decoder_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_DFT = DataLoader(dataset=dataset_DFT, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_AD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD = DataLoader(dataset=dataset_AD, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_RE = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE = DataLoader(dataset=dataset_RE, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_REI = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI = DataLoader(dataset=dataset_REI, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_AD_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD_MMD= DataLoader(dataset=dataset_AD_MMD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_RE_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE_MMD = DataLoader(dataset=dataset_RE_MMD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_REI_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI_MMD = DataLoader(dataset=dataset_REI_MMD, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_AD_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD_SWD = DataLoader(dataset=dataset_AD_SWD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_RE_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE_SWD = DataLoader(dataset=dataset_RE_SWD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_REI_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI_SWD = DataLoader(dataset=dataset_REI_SWD, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'009901'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_BF_SWD \u001b[39m=\u001b[39m test_dataset(\u001b[39m\"\u001b[39;49m\u001b[39m/home/github_io/Taiwan/BitFit/swd\u001b[39;49m\u001b[39m\"\u001b[39;49m,text_path)\n\u001b[1;32m      2\u001b[0m csmsc_dataloader_BF_SWD \u001b[39m=\u001b[39m DataLoader(dataset\u001b[39m=\u001b[39mdataset_BF_SWD, batch_size\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m dataset_BF_MMD \u001b[39m=\u001b[39m test_dataset(\u001b[39m\"\u001b[39m\u001b[39m/home/github_io/Taiwan/BitFit/mmd\u001b[39m\u001b[39m\"\u001b[39m,text_path)\n",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m, in \u001b[0;36mtest_dataset.__init__\u001b[0;34m(self, path, china_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m each \u001b[39min\u001b[39;00m \u001b[39mall\u001b[39m:\n\u001b[1;32m     21\u001b[0m         \u001b[39mid\u001b[39m, text \u001b[39m=\u001b[39m each\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m], each\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m         data[\u001b[39mid\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [text]\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(data\u001b[39m.\u001b[39mvalues()))\n",
      "\u001b[0;31mKeyError\u001b[0m: '009901'"
     ]
    }
   ],
   "source": [
    "dataset_BF_SWD = test_dataset(\"/home/github_io/Taiwan/BitFit/swd\",text_path)\n",
    "csmsc_dataloader_BF_SWD = DataLoader(dataset=dataset_BF_SWD, batch_size=50, shuffle=True)\n",
    "dataset_BF_MMD = test_dataset(\"/home/github_io/Taiwan/BitFit/mmd\",text_path)\n",
    "csmsc_dataloader_BF_MMD = DataLoader(dataset=dataset_BF_MMD, batch_size=50, shuffle=True)\n",
    "dataset_BF = test_dataset(\"/home/github_io/Taiwan/BitFit/wav\",text_path)\n",
    "csmsc_dataloader_BF = DataLoader(dataset=dataset_BF, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2023 17:59:34 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "import re\n",
    "model =  SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\")\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "                  \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "                  \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "                  \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csmsc_dataloader_FT, csmsc_dataloader_DFT, csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join([\"csmsc_dataloader_FT\", \"csmsc_dataloader_DFT\", \"csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (692595041.py, line 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 48\u001b[0;36m\u001b[0m\n\u001b[0;31m    predictions = transcriptions\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_metric\n",
    "# from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# wer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\n",
    "# cer = load_metric(\"cer.py\")\n",
    "\n",
    "# test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "# print(type(test_dataset))\n",
    "# processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "wer = []\n",
    "cer = []\n",
    "# for wav, text in china_dataloader:\n",
    "#     transcriptions = model.transcribe(wav)\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions )\n",
    "#     # print(f\"WER:{err['wer']}\")\n",
    "#     # print(f\"CER:{err['cer']}\")\n",
    "#     wer.append(err['wer'])\n",
    "#     cer.append(err['cer'])\n",
    "# print(f\"WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)}\")\n",
    "\n",
    "settings = [\"csmsc_dataloader_FT\", \"csmsc_dataloader_DFT\", \"csmsc_dataloader_AD\", \"csmsc_dataloader_RE\", \"csmsc_dataloader_REI\", \"csmsc_dataloader_AD_MMD\", \"csmsc_dataloader_AD_SWD\", \"csmsc_dataloader_RE_MMD\",  \"csmsc_dataloader_RE_SWD\", \"csmsc_dataloader_REI_MMD\",  \"csmsc_dataloader_REI_SWD\"]\n",
    "id=0\n",
    "print(len(settings))\n",
    "for csmsc_dataloader in [csmsc_dataloader_FT, csmsc_dataloader_DFT, csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD]:\n",
    "    wer = []\n",
    "    cer = []\n",
    "    wer_ = []\n",
    "    cer_ = []\n",
    "    for wav, text in csmsc_dataloader:\n",
    "        transcriptions = model.transcribe(wav)\n",
    "        # for i, predicted_sentence in enumerate(transcriptions):\n",
    "            # print(\"-\" * 100)\n",
    "            # print(\"Reference:\", text[i])\n",
    "            # print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription']\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        # print(f\"WER:{err['wer']}\")\n",
    "        # print(f\"CER:{err['cer']}\")\n",
    "        wer.append(err['wer'])\n",
    "        cer.append(err['cer'])\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "            # print(\"-\" * 100)\n",
    "            # print(\"Reference:\", text[i])\n",
    "            # print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription']\n",
    "            predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        wer_.append(err['wer'])\n",
    "        cer_.append(err['cer'])\n",
    "    print(f\"Settings:{settings[id]}, WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)},  WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "    id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2023 17:06:25 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
      "03/04/2023 17:06:36 - WARNING - root - bos_token <s> not in provided tokens. It will be added to the list of tokens\n",
      "03/04/2023 17:06:36 - WARNING - root - eos_token </s> not in provided tokens. It will be added to the list of tokens\n"
     ]
    }
   ],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "model =  SpeechRecognitionModel(\"StevenLimcorn/wav2vec2-xls-r-300m-zh-TW\")\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "                  \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "                  \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "                  \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF_MMD, WER: 0.8417721518987342, CER: 0.36493738819320215,  WER_: 0.8417721518987342, CER_: 0.36493738819320215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF_SWD, WER: 0.8417721518987342, CER: 0.37209302325581395,  WER_: 0.8417721518987342, CER_: 0.37209302325581395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF, WER: 0.8291139240506329, CER: 0.38640429338103754,  WER_: 0.8291139240506329, CER_: 0.38640429338103754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "wer = []\n",
    "cer = []\n",
    "wer_ = []\n",
    "cer_ = []\n",
    "# for wav, text in china_dataloader:\n",
    "#     transcriptions = model.transcribe(wav)\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription'])\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions)\n",
    "#     # print(f\"WER:{err['wer']}\")\n",
    "#     # print(f\"CER:{err['cer']}\")\n",
    "#     wer.append(err['wer'])\n",
    "#     cer.append(err['cer'])\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription']).replace(\" \",\"\")\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions)\n",
    "#     wer_.append(err['wer'])\n",
    "#     cer_.append(err['cer'])\n",
    "# print(f\"WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)}, WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "\n",
    "\n",
    "settings = [\"csmsc_dataloader_BF_MMD\", \"csmsc_dataloader_BF_SWD\", \"csmsc_dataloader_BF\"]\n",
    "id=0\n",
    "for csmsc_dataloader in [csmsc_dataloader_BF_MMD, csmsc_dataloader_BF_SWD, csmsc_dataloader_BF]:\n",
    "    wer = []\n",
    "    cer = []\n",
    "    wer_ = []\n",
    "    cer_ = []\n",
    "    for wav, text in csmsc_dataloader:\n",
    "        transcriptions = model.transcribe(wav)\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "            # print(\"-\" * 100)\n",
    "            # print(\"Reference:\", text[i])\n",
    "            # print(\"Prediction:\", converter.convert(predicted_sentence['transcription']), predicted_sentence['transcription'])\n",
    "            predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription'])\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        # print(f\"WER:{err['wer']}\")\n",
    "        # print(f\"CER:{err['cer']}\")\n",
    "        wer.append(err['wer'])\n",
    "        cer.append(err['cer'])\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "        #     print(\"-\" * 100)\n",
    "        #     print(\"Reference:\", text[i])\n",
    "        #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription']).replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions)\n",
    "        wer_.append(err['wer'])\n",
    "        cer_.append(err['cer'])\n",
    "    print(f\"Settings:{settings[id]}, WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)},  WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "    id+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "espnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2996fb8a4672b75c21a5867181c07f87a485c7d904fce480a188f0832038e788"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
