{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "class china_accent(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,path):\n",
    "        self.path = path\n",
    "        data = {}\n",
    "        with open(os.path.join(self.path,\"wav.scp\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, wav_path = each.split(\" \")[0], each.split(\" \")[1].strip(\"\\n\")\n",
    "                data[id] = [wav_path]\n",
    "        with open(os.path.join(self.path,\"text\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, text = each.split(\"\\t\")[0], each.split(\"\\t\")[1].strip(\"\\n\")\n",
    "                data[id] += [text]\n",
    "        self.n_samples = len(data.keys())\n",
    "        self.data = np.array(list(data.values()))\n",
    "        # print(self.data)\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return wav_path, text\n",
    "        '''\n",
    "        # speech_array, sampling_rate = librosa.load(self.data[index][0], sr=16_000)\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class test_dataset(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,path, china_path):\n",
    "        self.path = path\n",
    "        self.china_path = china_path ## for char not phn\n",
    "        data = {}\n",
    "        # with open(os.path.join(self.path,\"wav.scp\"),'r') as f:\n",
    "        #     all =  f.readlines()\n",
    "        #     for each in all:\n",
    "        #         id, wav_path = each.split(\" \")[0], each.split(\" \")[1].strip(\"\\n\")\n",
    "        #         data[id] = [wav_path]\n",
    "        for each in os.listdir(self.path):\n",
    "            # print(each)\n",
    "            if \".wav\" in  each:\n",
    "                id, wav_path = each.replace(\".wav\",\"\"), os.path.join(self.path,each)\n",
    "                data[id] = [wav_path]\n",
    "        with open(os.path.join(self.china_path,\"text\"),'r') as f:\n",
    "            all =  f.readlines()\n",
    "            for each in all:\n",
    "                id, text = each.split(\"\\t\")[0], each.split(\"\\t\")[1].strip(\"\\n\")\n",
    "                data[id] += [text]\n",
    "        self.n_samples = len(data.keys())\n",
    "        self.data = np.array(list(data.values()))\n",
    "        # print(self.data)\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        return wav_path, text\n",
    "        '''\n",
    "        # speech_array, sampling_rate = librosa.load(self.data[index][0], sr=16_000)\n",
    "        return self.data[index][0], self.data[index][1]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = \"/home/espnet/egs2/csmsc/tts1/data/eval1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_FT = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_FT = DataLoader(dataset=dataset_FT, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_DFT = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_decoder_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_DFT = DataLoader(dataset=dataset_DFT, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_AD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD = DataLoader(dataset=dataset_AD, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_RE = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE = DataLoader(dataset=dataset_RE, batch_size=100, shuffle=True)\n",
    "\n",
    "dataset_REI = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI = DataLoader(dataset=dataset_REI, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_AD_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD_MMD= DataLoader(dataset=dataset_AD_MMD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_RE_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE_MMD = DataLoader(dataset=dataset_RE_MMD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_REI_MMD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_mmd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI_MMD = DataLoader(dataset=dataset_REI_MMD, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_AD_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_adapter_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_AD_SWD = DataLoader(dataset=dataset_AD_SWD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_RE_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_RE_SWD = DataLoader(dataset=dataset_RE_SWD, batch_size=50, shuffle=True)\n",
    "\n",
    "dataset_REI_SWD = test_dataset(\"/home/espnet/egs2/csmsc/tts1/exp/tts_finetune_fastpeech2_reprogram_input_swd_hlr/decode_fastspeech_train.loss.ave/eval1/wav\",text_path)\n",
    "csmsc_dataloader_REI_SWD = DataLoader(dataset=dataset_REI_SWD, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_BF_SWD = test_dataset(\"/home/github_io/Taiwan/BitFit/swd\",text_path)\n",
    "csmsc_dataloader_BF_SWD = DataLoader(dataset=dataset_BF_SWD, batch_size=50, shuffle=True)\n",
    "dataset_BF_MMD = test_dataset(\"/home/github_io/Taiwan/BitFit/mmd\",text_path)\n",
    "csmsc_dataloader_BF_MMD = DataLoader(dataset=dataset_BF_MMD, batch_size=50, shuffle=True)\n",
    "dataset_BF = test_dataset(\"/home/github_io/Taiwan/BitFit/wav\",text_path)\n",
    "csmsc_dataloader_BF = DataLoader(dataset=dataset_BF, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2023 17:39:38 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/configuration_utils.py:375: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "import re\n",
    "model =  SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\")\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "                  \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "                  \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "                  \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csmsc_dataloader_FT, csmsc_dataloader_DFT, csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join([\"csmsc_dataloader_FT\", \"csmsc_dataloader_DFT\", \"csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:04<00:39,  2.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m cer_ \u001b[39m=\u001b[39m []\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m wav, text \u001b[39min\u001b[39;00m csmsc_dataloader:\n\u001b[0;32m---> 41\u001b[0m     transcriptions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtranscribe(wav)\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m i, predicted_sentence \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(transcriptions):\n\u001b[1;32m     43\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/huggingsound/speech_recognition/model.py:125\u001b[0m, in \u001b[0;36mSpeechRecognitionModel.transcribe\u001b[0;34m(self, paths, batch_size, decoder)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(inputs, \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 125\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(inputs\u001b[39m.\u001b[39;49minput_values\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice),attention_mask\u001b[39m=\u001b[39;49minputs\u001b[39m.\u001b[39;49mattention_mask\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice))\u001b[39m.\u001b[39mlogits\n\u001b[1;32m    126\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs\u001b[39m.\u001b[39minput_values\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1680\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[39m    Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[39m    config.vocab_size - 1]`.\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1680\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwav2vec2(\n\u001b[1;32m   1681\u001b[0m     input_values,\n\u001b[1;32m   1682\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1683\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1684\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1685\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1686\u001b[0m )\n\u001b[1;32m   1688\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1689\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1315\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1310\u001b[0m hidden_states, extract_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1311\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1312\u001b[0m     hidden_states, mask_time_indices\u001b[39m=\u001b[39mmask_time_indices, attention_mask\u001b[39m=\u001b[39mattention_mask\n\u001b[1;32m   1313\u001b[0m )\n\u001b[0;32m-> 1315\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1316\u001b[0m     hidden_states,\n\u001b[1;32m   1317\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1318\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1319\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1320\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1321\u001b[0m )\n\u001b[1;32m   1323\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:857\u001b[0m, in \u001b[0;36mWav2Vec2EncoderStableLayerNorm.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    852\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mfinfo(hidden_states\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin\n\u001b[1;32m    853\u001b[0m     attention_mask \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39mexpand(\n\u001b[1;32m    854\u001b[0m         attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], attention_mask\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    855\u001b[0m     )\n\u001b[0;32m--> 857\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_conv_embed(hidden_states)\n\u001b[1;32m    858\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m position_embeddings\n\u001b[1;32m    859\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:387\u001b[0m, in \u001b[0;36mWav2Vec2PositionalConvEmbedding.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    385\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m--> 387\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[1;32m    388\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding(hidden_states)\n\u001b[1;32m    389\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1118\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 1120\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1121\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1122\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/conv.py:301\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/espnet/lib/python3.8/site-packages/torch/nn/modules/conv.py:297\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    295\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    296\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 297\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    298\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "from datasets import load_metric\n",
    "# from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# wer = load_metric(\"wer.py\") # https://github.com/jonatasgrosman/wav2vec2-sprint/blob/main/wer.py\n",
    "# cer = load_metric(\"cer.py\")\n",
    "\n",
    "# test_dataset = load_dataset(\"common_voice\", LANG_ID, split=f\"test[:{SAMPLES}]\")\n",
    "# print(type(test_dataset))\n",
    "# processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "wer = []\n",
    "cer = []\n",
    "# for wav, text in china_dataloader:\n",
    "#     transcriptions = model.transcribe(wav)\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions )\n",
    "#     # print(f\"WER:{err['wer']}\")\n",
    "#     # print(f\"CER:{err['cer']}\")\n",
    "#     wer.append(err['wer'])\n",
    "#     cer.append(err['cer'])\n",
    "# print(f\"WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)}\")\n",
    "\n",
    "settings = [\"csmsc_dataloader_FT\", \"csmsc_dataloader_DFT\", \"csmsc_dataloader_AD\", \"csmsc_dataloader_RE\", \"csmsc_dataloader_REI\", \"csmsc_dataloader_AD_MMD\", \"csmsc_dataloader_AD_SWD\", \"csmsc_dataloader_RE_MMD\",  \"csmsc_dataloader_RE_SWD\", \"csmsc_dataloader_REI_MMD\",  \"csmsc_dataloader_REI_SWD\"]\n",
    "id=0\n",
    "print(len(settings))\n",
    "for csmsc_dataloader in [csmsc_dataloader_FT, csmsc_dataloader_DFT, csmsc_dataloader_AD, csmsc_dataloader_RE, csmsc_dataloader_REI, csmsc_dataloader_AD_MMD, csmsc_dataloader_AD_SWD, csmsc_dataloader_RE_MMD,  csmsc_dataloader_RE_SWD, csmsc_dataloader_REI_MMD,  csmsc_dataloader_REI_SWD]:\n",
    "    wer = []\n",
    "    cer = []\n",
    "    wer_ = []\n",
    "    cer_ = []\n",
    "    for wav, text in csmsc_dataloader:\n",
    "        transcriptions = model.transcribe(wav)\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "            print(\"-\" * 100)\n",
    "            print(\"Reference:\", text[i])\n",
    "            print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription']\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        # print(f\"WER:{err['wer']}\")\n",
    "        # print(f\"CER:{err['cer']}\")\n",
    "        wer.append(err['wer'])\n",
    "        cer.append(err['cer'])\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "            # print(\"-\" * 100)\n",
    "            # print(\"Reference:\", text[i])\n",
    "            # print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            # predicted_sentence['transcription'] = predicted_sentence['transcription']\n",
    "            predicted_sentence['transcription'] = predicted_sentence['transcription'].replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        wer_.append(err['wer'])\n",
    "        cer_.append(err['cer'])\n",
    "    print(f\"Settings:{settings[id]}, WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)},  WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "    id +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2023 17:06:25 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
      "03/04/2023 17:06:36 - WARNING - root - bos_token <s> not in provided tokens. It will be added to the list of tokens\n",
      "03/04/2023 17:06:36 - WARNING - root - eos_token </s> not in provided tokens. It will be added to the list of tokens\n"
     ]
    }
   ],
   "source": [
    "import opencc\n",
    "converter = opencc.OpenCC('t2s.json')\n",
    "model =  SpeechRecognitionModel(\"StevenLimcorn/wav2vec2-xls-r-300m-zh-TW\")\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the audio files as arrays\n",
    "CHARS_TO_IGNORE = [\",\", \"?\", \"¿\", \".\", \"!\", \"¡\", \";\", \"；\", \":\", '\"\"', \"%\", '\"', \"�\", \"ʿ\", \"·\", \"჻\", \"~\", \"՞\",\n",
    "                  \"؟\", \"،\", \"।\", \"॥\", \"«\", \"»\", \"„\", \"“\", \"”\", \"「\", \"」\", \"‘\", \"’\", \"《\", \"》\", \"(\", \")\", \"[\", \"]\",\n",
    "                  \"{\", \"}\", \"=\", \"`\", \"_\", \"+\", \"<\", \">\", \"…\", \"–\", \"°\", \"´\", \"ʾ\", \"‹\", \"›\", \"©\", \"®\", \"—\", \"→\", \"。\",\n",
    "                  \"、\", \"﹂\", \"﹁\", \"‧\", \"～\", \"﹏\", \"，\", \"｛\", \"｝\", \"（\", \"）\", \"［\", \"］\", \"【\", \"】\", \"‥\", \"〽\",\n",
    "                  \"『\", \"』\", \"〝\", \"〟\", \"⟨\", \"⟩\", \"〜\", \"：\", \"！\", \"？\", \"♪\", \"؛\", \"/\", \"\\\\\", \"º\", \"−\", \"^\", \"'\", \"ʻ\", \"ˆ\"]\n",
    "chars_to_ignore_regex = f\"[{re.escape(''.join(CHARS_TO_IGNORE))}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF_MMD, WER: 0.8417721518987342, CER: 0.36493738819320215,  WER_: 0.8417721518987342, CER_: 0.36493738819320215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF_SWD, WER: 0.8417721518987342, CER: 0.37209302325581395,  WER_: 0.8417721518987342, CER_: 0.37209302325581395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:13<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings:taiwan_dataloader_BF, WER: 0.8291139240506329, CER: 0.38640429338103754,  WER_: 0.8291139240506329, CER_: 0.38640429338103754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "wer = []\n",
    "cer = []\n",
    "wer_ = []\n",
    "cer_ = []\n",
    "# for wav, text in china_dataloader:\n",
    "#     transcriptions = model.transcribe(wav)\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription'])\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions)\n",
    "#     # print(f\"WER:{err['wer']}\")\n",
    "#     # print(f\"CER:{err['cer']}\")\n",
    "#     wer.append(err['wer'])\n",
    "#     cer.append(err['cer'])\n",
    "#     for i, predicted_sentence in enumerate(transcriptions):\n",
    "#     #     print(\"-\" * 100)\n",
    "#     #     print(\"Reference:\", text[i])\n",
    "#     #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "#         predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription']).replace(\" \",\"\")\n",
    "#     predictions = transcriptions\n",
    "#     references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "#     err = model.evaluate(references,predictions)\n",
    "#     wer_.append(err['wer'])\n",
    "#     cer_.append(err['cer'])\n",
    "# print(f\"WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)}, WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "\n",
    "\n",
    "settings = [\"csmsc_dataloader_BF_MMD\", \"csmsc_dataloader_BF_SWD\", \"csmsc_dataloader_BF\"]\n",
    "id=0\n",
    "for csmsc_dataloader in [csmsc_dataloader_BF_MMD, csmsc_dataloader_BF_SWD, csmsc_dataloader_BF]:\n",
    "    wer = []\n",
    "    cer = []\n",
    "    wer_ = []\n",
    "    cer_ = []\n",
    "    for wav, text in csmsc_dataloader:\n",
    "        transcriptions = model.transcribe(wav)\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "            # print(\"-\" * 100)\n",
    "            # print(\"Reference:\", text[i])\n",
    "            # print(\"Prediction:\", converter.convert(predicted_sentence['transcription']), predicted_sentence['transcription'])\n",
    "            predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription'])\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions )\n",
    "        # print(f\"WER:{err['wer']}\")\n",
    "        # print(f\"CER:{err['cer']}\")\n",
    "        wer.append(err['wer'])\n",
    "        cer.append(err['cer'])\n",
    "        for i, predicted_sentence in enumerate(transcriptions):\n",
    "        #     print(\"-\" * 100)\n",
    "        #     print(\"Reference:\", text[i])\n",
    "        #     print(\"Prediction:\", predicted_sentence['transcription'])\n",
    "            predicted_sentence['transcription'] = converter.convert(predicted_sentence['transcription']).replace(\" \",\"\")\n",
    "        predictions = transcriptions\n",
    "        references = [{\"transcription\":re.sub(chars_to_ignore_regex, \"\", x).upper()} for x in text]\n",
    "        err = model.evaluate(references,predictions)\n",
    "        wer_.append(err['wer'])\n",
    "        cer_.append(err['cer'])\n",
    "    print(f\"Settings:{settings[id]}, WER: {sum(wer)/len(wer)}, CER: {sum(cer)/len(cer)},  WER_: {sum(wer_)/len(wer_)}, CER_: {sum(cer_)/len(cer_)}\")\n",
    "    id+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "espnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2996fb8a4672b75c21a5867181c07f87a485c7d904fce480a188f0832038e788"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
